[{"authors":[],"categories":null,"content":"","date":1555525800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555525800,"objectID":"ec0f9ca8976ec647a7cff0b4434e29cb","permalink":"/talk/hardnessvsrandomness/","publishdate":"2019-04-18T00:00:00+05:30","relpermalink":"/talk/hardnessvsrandomness/","section":"talk","summary":"","tags":[],"title":"Hardness vs. Randomness","type":"talk"},{"authors":["Rahul Gupta","**Shubham Sharma**","Subhajit Roy","and Kuldeep S. Meel"],"categories":null,"content":"","date":1554336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554336000,"objectID":"07e917a43bdd2d7a9480a76264d996b3","permalink":"/publication/waps/","publishdate":"2019-04-04T00:00:00Z","relpermalink":"/publication/waps/","section":"publication","summary":"Given a set of constraints $F$ and a user-defined weight function $W$ on the assignment space, the problem of constrained sampling is to sample satisfying assignments of $F$ conditioned on $W$. Constrained sampling is a fundamental problem with applications in probabilistic reasoning, synthesis, software and hardware testing. Consequently, the problem of sampling has been subject to intense theoretical and practical investigations over the years. Despite such intense investigations, there still remains a gap between theory and practice. In particular, there has been significant progress in the development of sampling techniques when $W$ is a uniform distribution, but such techniques fail to handle general weight functions $W$. Furthermore, we are, often, interested in $Sigma_1^1$ formulas, i.e., $G(X):= exists Y F(X, Y)$ for some $F$; typically the set of variables $Y$ are introduced as auxiliary variables during encoding of constraints to $F$.  In this context, one wonders  whether it is possible to design sampling techniques whose runtime performance is agnostic to the underlying weight distribution and can handle $Sigma_1^1$ formulas? The primary contribution of this work is a novel technique, called WAPS, for sampling over $Sigma_1^1$ whose runtime is agnostic to $W$. {WAPS} is based on our recently discovered connection between knowledge compilation and uniform sampling. WAPS proceeds by compiling $F$ into a well studied compiled form, d-DNNF, which allows sampling operations to be conducted in linear time in the size of the compiled form.  We demonstrate that WAPS can significantly outperform existing state-of-the-art weighted and projected sampler WeightGen, by up to 3 orders of magnitude in runtime while achieving a geometric speedup of $296\times$ and solving $564$ more instances out of $773$. The distribution generated by WAPS is statistically indistinguishable from that generated by an ideal weighted and projected sampler. Furthermore, WAPS is almost oblivious to the number of samples requested.","tags":null,"title":" WAPS: Weighted and Projected Sampling","type":"publication"},{"authors":[],"categories":[],"content":"This blogpost is based on our (Myself, Rahul, Subhajit and Kuldeep) paper that got published in the procedings of International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR), 2018. The code is available here. The primary contribution of this work is marrying knowledge compilation with uniform sampling to design a new uniform sampler KUS. The main result is that KUS to solve more number of benchmarks than existing state-of-the art uniform and almost-uniform samplers beating them by orders of magnitude in terms of runtime: Uniform Sampling\nGiven a boolean formula $\\phi$, the idea of Uniform Sampling is to generate samples from the set of solutions of $F$ called $R_F$ using a generator $\\mathcal{G}$ that guarantees: $$\\forall y \\in R_F, \\mathsf{Pr}\\left[\\mathcal{G}(F) = y\\right] = \\frac{1}{|R_F|},$$ Uniform sampling is a fundamental problem in computer science with wide range of applications ranging from Bayesian analysis to software engineering and programming languages. Jerrum, Valiant, and Vazirani observed deep relationship between model counting and uniform sampling. In particular, they showed that given access to an exact model counter, one could design a uniform generator which requires only polynomially many queries to the exact model counter. On the other hand, knowledge compilation has been emerged as a vital task wherein a logical theory is compiled into a form that allows performing probabilistic inference in polynomial time. It is well known that there is a deep connection between probabilistic inference and model counting. In this context, one wonders if the recent advances in knowledge compilation can be harnessed to design a scalable uniform sampler. The primary contribution of this work is marrying knowledge compilation with uniform sampling to design a new algorithm, KUS, that performs uniform sampling, outperforming current state-of-the-art approximately uniform and uniform samplers.\n Knowledge Compilation and d-DNNF representation\nTo deal with computational intractability of probabilistic reasoning, knowledge compilation seeks to compile a knowledge base, often represented as a propositional formula in CNF, to a target language. Thereafter, probabilistic reasoning tasks, which are often expressed as sequence of queries, are performed by querying the knowledge base in the target language. Deterministic Decomposable Negation Normal Form (d-DNNF) have emerged as a central target language in knowledge compilation community since several probabilistic reasoning tasks such as probabilistic inference, maximum a posteriori (MAP) can be answered in polynomial time in the size of d-DNNF. A boolean formula in Negation Normal Form (NNF) is said to be in d-DNNF if it satisfes the following properties:   Deterministic: We refer to an NNF as deterministic if the operands of OR in all wellformed Boolean formula in the NNF are mutually inconsistent. Decomposable: We refer to an NNF as decomposable if the operands of AND in all wellformed Boolean formula in the NNF are expressed in a mutually disjoint set of variables.  d-DNNF of a boolean formula $F$ represent the set of satisfying assignment $R_F$ The algorithm\nThe central idea behind KUS is to first employ the state-of-the-art knowledge compilation approaches to compile a given CNF formula into d-DNNF form, and then performing only two passes over the d-DNNF representation to generate as many identically and independently distributed samples as specified by the user denoted by $s$. KUS takes in a CNF formula $F$ and required number of samples s and returns a set of $s$ samples such that each sample is uniformly and independently drawn from the uniform distribution over the set of solutions $R_F$. KUS first invokes a d-DNNF compiler over the formula F to obtain its d-DNNF. Then, the subroutine Annotate is invoked that annotates d-DNNF by annotating each node with a tuple consisting of the number of solutions and the set of variables in the node\u0026rsquo;s corresponding sub-formula. Then, the subroutine Sampler is invoked that returns s uniformly and independently drawn samples using the properties of d-DNNF. Finally, KUS gives random assignment to the unassigned variables for each sample in the SampleList to account for unconstrained variables that do not appear in d-DNNF by invoking the subroutine RandomAssignment. The Results Our experiments demonstrated that KUS outperformed both SPUR and UniGen2 state-of-the-art uniform and almost-uniform samplers by a factor of up to 3 orders of magnitude in terms of runtime in some cases while achieving a geometric speedup of $1.7\\times$ and $8.3\\times$ over SPUR and UniGen2 respectively. The distribution generated by KUS is statistically indistinguishable from that generated by an ideal uniform sampler. Moreover, KUS is almost oblivious to the number of samples requested. Finally, we observe that KUS can benefit from different d-DNNF compilers, therefore suggesting development of portfolio samplers in future. One of the biggest advantage of KUS is in incremental sampling\u0026ndash;fetching multiple, relatively small-sized samples, repeatedly. The typical use case of iterative sampling can be in repeated invocation of a sampling tool until the objective (such as desired coverage or violation of property) is achieved. In incremental-sampling KUS achieves speedups of upto 3 orders of magnitude. Conclusion\n In this word, we have proposed a new approach for uniform sampling that builds on breakthrough progress in knowledge compilation Experimentally we have demonstrated that KUS outperformed state-of-the-art uniform and almost-uniform samplers We believe that the success of KUS will motivate researchers in verification and knowledge compilation communities to investigate a broader set of logical forms amenable to efficient uniform generation  ","date":1552413123,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552413123,"objectID":"e22e5d18dcda6b87e515a3cd2f8ecddb","permalink":"/post/kus/","publishdate":"2019-03-12T23:22:03+05:30","relpermalink":"/post/kus/","section":"post","summary":"This blogpost is based on our (Myself, Rahul, Subhajit and Kuldeep) paper that got published in the procedings of International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR), 2018. The code is available here. The primary contribution of this work is marrying knowledge compilation with uniform sampling to design a new uniform sampler KUS. The main result is that KUS to solve more number of benchmarks than existing state-of-the art uniform and almost-uniform samplers beating them by orders of magnitude in terms of runtime: Uniform Sampling","tags":["Uniform Sampling","Knowledge Compilation","Model counting"],"title":"Knowledge Compilation meets Uniform Sampling","type":"post"},{"authors":["Shubham S. Srivastava","Medha Atre","**Shubham Sharma**","Rahul Gupta","Sandeep K. Shukla"],"categories":null,"content":"","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"5b6f9e1f6e16317e451a82f3472e33e6","permalink":"/publication/verity/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/publication/verity/","section":"publication","summary":"Integrity and security of the data in database systems are typically maintained with access control policies and firewalls. However, insider attacks -- where someone with an intimate knowledge of the system and administrative privileges tampers with the data -- pose a unique challenge. Measures like append only logging prove to be insufficient because an attacker with administrative privileges can alter logs and login records to eliminate the trace of attack, thus making insider attacks hard to detect. In this paper, we propose Verity -- first of a kind system to the best of our knowledge. Verity serves as a dataless framework by which any blockchain network can be used to store fixed-length metadata about tuples from any SQL database, without complete migration of the database. Verity uses a formalism for parsing SQL queries and query results to check the respective tuples' integrity using blockchains to detect insider attacks. We have implemented our technique using Hyperledger Fabric, Composer REST API, and SQLite database. Using TPC-H data and SQL queries of varying complexity and types, our experiments demonstrate that any overhead of integrity checking remains constant per tuple in a query's results, and scales linearly.","tags":null,"title":"Verity: Blockchains to Detect Insider Attacks in DBMS","type":"publication"},{"authors":["**Shubham Sharma**","Rahul Gupta","Subhajit Roy","and Kuldeep S. Meel"],"categories":null,"content":"","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"99a004cca03ade7b5449ccfb037ebcd4","permalink":"/publication/kus/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/kus/","section":"publication","summary":"Uniform sampling has drawn diverse applications in programming languages and software engineering, like in constrained-random verification (CRV), constrained-fuzzing and bug synthesis. The effectiveness of these applications depend on the uniformity of test stimuli generated from a given set of constraints. Despite significant progress over the past few years, the performance of the state of the art techniques still falls short of those of heuristic methods employed in the industry which sacrifice either uniformity or scalability when generating stimuli. In this paper, we propose a new approach to the uniform generation that builds on recent progress in knowledge compilation. The primary contribution of this paper is marrying knowledge compilation with uniform sampling: our algorithm, KUS, employs the state-of-the-art knowledge compilers to first compile constraints into d-DNNF form, and then, generates samples by making two passes over the compiled representation. We show that KUS is able to significantly outperform existing state-of-the-art algorithms, SPUR and UniGen2, by up to 3 orders of magnitude in terms of runtime while achieving a geometric speedup of 1.7 and 8.3 over SPUR and UniGen2 respectively. Also, KUS achieves a lower PAR-2 score, around 0.82x that of SPUR and 0.38x that of UniGen2. Furthermore, KUS achieves speedups of up to 3 orders of magnitude for incremental sampling. The distribution generated by KUS is statistically indistinguishable from that generated by an ideal uniform sampler. Moreover, KUS is almost oblivious to the number of samples requested.","tags":null,"title":"Knowledge Compilation meets Uniform Sampling ","type":"publication"},{"authors":["**Shubham Sharma**","Rahul Gupta","Shubham S. Srivastava","Sandeep K. Shukla"],"categories":null,"content":"","date":1513468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513468800,"objectID":"38c4d205593a3a39e6e09a940b36b0c7","permalink":"/publication/db-blockchain/","publishdate":"2017-12-17T00:00:00Z","relpermalink":"/publication/db-blockchain/","section":"publication","summary":"Applications relying on centralized databases are often vulnerable to insider attacks. Any user with administrative privileges to the database system or the hosting server, is capable of modifying the database entries. Furthermore, such a user might modify the corresponding log entries, making it extremely difficult to detect such an attack. Attribution of the attack to privileged users would also be challenging. In this paper, we propose a solution to this problem using the tamperresistance property of blockchains. We have implemented and tested our protocol, using multichain, on a web-based online academic grading database application with a goal to make it robust to such an insider attack.","tags":null,"title":"Detecting Insider Attacks on Databases using Blockchains","type":"publication"}]